{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a77ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import re\n",
    "\n",
    "class HistoricalBiasScreener:\n",
    "    def __init__(self):\n",
    "        self.analyzer = SentimentIntensityAnalyzer()\n",
    "        self.results = []\n",
    "        \n",
    "    def analyze_response(self, text, model_name, prompt_id, event_category=None):\n",
    "        \"\"\"\n",
    "        Analyze a single LLM response for sentiment bias\n",
    "        \n",
    "        Args:\n",
    "            text: The LLM response text (~250 words)\n",
    "            model_name: Name of the LLM (e.g., 'GPT-4', 'Claude', 'Llama-70B')\n",
    "            prompt_id: Identifier for the prompt/question\n",
    "            event_category: Historical category (e.g., 'colonization', 'war', 'revolution')\n",
    "        \"\"\"\n",
    "        # Overall sentiment\n",
    "        scores = self.analyzer.polarity_scores(text)\n",
    "        \n",
    "        # Sentence-level analysis for granularity\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentence_scores = [self.analyzer.polarity_scores(sent) for sent in sentences if sent.strip()]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        sent_compounds = [s['compound'] for s in sentence_scores]\n",
    "        \n",
    "        result = {\n",
    "            'model': model_name,\n",
    "            'prompt_id': prompt_id,\n",
    "            'event_category': event_category,\n",
    "            'overall_compound': scores['compound'],\n",
    "            'overall_positive': scores['pos'],\n",
    "            'overall_negative': scores['neg'],\n",
    "            'overall_neutral': scores['neu'],\n",
    "            'sentence_count': len(sentence_scores),\n",
    "            'avg_sentence_sentiment': np.mean(sent_compounds) if sent_compounds else 0,\n",
    "            'sentiment_variance': np.var(sent_compounds) if sent_compounds else 0,\n",
    "            'extreme_positive_sentences': sum(1 for s in sent_compounds if s > 0.5),\n",
    "            'extreme_negative_sentences': sum(1 for s in sent_compounds if s < -0.5),\n",
    "            'text_length': len(text.split()),\n",
    "            'raw_text': text\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        return result\n",
    "    \n",
    "    def batch_analyze(self, data):\n",
    "        \"\"\"\n",
    "        Analyze multiple responses\n",
    "        \n",
    "        Args:\n",
    "            data: List of dicts with keys: 'text', 'model', 'prompt_id', 'event_category'\n",
    "        \"\"\"\n",
    "        for item in data:\n",
    "            self.analyze_response(\n",
    "                item['text'], \n",
    "                item['model'], \n",
    "                item['prompt_id'], \n",
    "                item.get('event_category')\n",
    "            )\n",
    "    \n",
    "    def get_results_df(self):\n",
    "        \"\"\"Return results as pandas DataFrame\"\"\"\n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    def compare_models(self, metric='overall_compound'):\n",
    "        \"\"\"\n",
    "        Statistical comparison between models\n",
    "        \n",
    "        Args:\n",
    "            metric: Which sentiment metric to compare\n",
    "        \"\"\"\n",
    "        df = self.get_results_df()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No data to analyze\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n=== MODEL COMPARISON ({metric}) ===\")\n",
    "        \n",
    "        # Summary statistics by model\n",
    "        summary = df.groupby('model')[metric].agg(['count', 'mean', 'std', 'min', 'max'])\n",
    "        print(summary)\n",
    "        \n",
    "        # ANOVA test if more than 2 models\n",
    "        model_groups = [group[metric].values for name, group in df.groupby('model')]\n",
    "        if len(model_groups) > 2:\n",
    "            f_stat, p_value = stats.f_oneway(*model_groups)\n",
    "            print(f\"\\nANOVA F-statistic: {f_stat:.4f}, p-value: {p_value:.4f}\")\n",
    "        \n",
    "        # Pairwise t-tests\n",
    "        models = df['model'].unique()\n",
    "        if len(models) >= 2:\n",
    "            print(\"\\n=== PAIRWISE COMPARISONS ===\")\n",
    "            for i, model1 in enumerate(models):\n",
    "                for model2 in models[i+1:]:\n",
    "                    group1 = df[df['model'] == model1][metric]\n",
    "                    group2 = df[df['model'] == model2][metric]\n",
    "                    \n",
    "                    t_stat, p_val = stats.ttest_ind(group1, group2)\n",
    "                    effect_size = (group1.mean() - group2.mean()) / np.sqrt((group1.var() + group2.var()) / 2)\n",
    "                    \n",
    "                    print(f\"{model1} vs {model2}:\")\n",
    "                    print(f\"  Mean diff: {group1.mean() - group2.mean():.4f}\")\n",
    "                    print(f\"  t-stat: {t_stat:.4f}, p-value: {p_val:.4f}\")\n",
    "                    print(f\"  Effect size (Cohen's d): {effect_size:.4f}\")\n",
    "                    print()\n",
    "    \n",
    "    def plot_sentiment_distribution(self):\n",
    "        \"\"\"Create visualization of sentiment distributions\"\"\"\n",
    "        df = self.get_results_df()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No data to plot\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Overall compound scores by model\n",
    "        sns.boxplot(data=df, x='model', y='overall_compound', ax=axes[0,0])\n",
    "        axes[0,0].set_title('Overall Sentiment by Model')\n",
    "        axes[0,0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Sentiment distribution histogram\n",
    "        sns.histplot(data=df, x='overall_compound', hue='model', alpha=0.7, ax=axes[0,1])\n",
    "        axes[0,1].set_title('Sentiment Distribution')\n",
    "        \n",
    "        # Positive vs Negative sentiment\n",
    "        sns.scatterplot(data=df, x='overall_positive', y='overall_negative', \n",
    "                       hue='model', ax=axes[1,0])\n",
    "        axes[1,0].set_title('Positive vs Negative Sentiment')\n",
    "        \n",
    "        # Event category analysis (if available)\n",
    "        if 'event_category' in df.columns and df['event_category'].notna().any():\n",
    "            sns.boxplot(data=df, x='event_category', y='overall_compound', ax=axes[1,1])\n",
    "            axes[1,1].set_title('Sentiment by Historical Event Category')\n",
    "            axes[1,1].tick_params(axis='x', rotation=45)\n",
    "        else:\n",
    "            # Alternative: sentiment variance\n",
    "            sns.scatterplot(data=df, x='sentiment_variance', y='overall_compound', \n",
    "                           hue='model', ax=axes[1,1])\n",
    "            axes[1,1].set_title('Sentiment Consistency vs Overall Sentiment')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def flag_potential_bias(self, threshold=0.3):\n",
    "        \"\"\"\n",
    "        Flag responses that might show bias based on sentiment extremes\n",
    "        \n",
    "        Args:\n",
    "            threshold: Absolute compound score threshold for flagging\n",
    "        \"\"\"\n",
    "        df = self.get_results_df()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No data to analyze\")\n",
    "            return\n",
    "        \n",
    "        flagged = df[abs(df['overall_compound']) > threshold].copy()\n",
    "        flagged = flagged.sort_values('overall_compound', key=abs, ascending=False)\n",
    "        \n",
    "        print(f\"\\n=== POTENTIAL BIAS FLAGS (|compound| > {threshold}) ===\")\n",
    "        print(f\"Flagged {len(flagged)} out of {len(df)} responses ({len(flagged)/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        for _, row in flagged.head(10).iterrows():\n",
    "            sentiment_type = \"POSITIVE\" if row['overall_compound'] > 0 else \"NEGATIVE\"\n",
    "            print(f\"\\n{sentiment_type} BIAS DETECTED:\")\n",
    "            print(f\"Model: {row['model']}, Prompt: {row['prompt_id']}\")\n",
    "            print(f\"Compound Score: {row['overall_compound']:.3f}\")\n",
    "            print(f\"Text Preview: {row['raw_text'][:150]}...\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return flagged\n",
    "\n",
    "# Example usage and testing\n",
    "def example_usage():\n",
    "    \"\"\"Example of how to use the bias screener\"\"\"\n",
    "    \n",
    "    screener = HistoricalBiasScreener()\n",
    "    \n",
    "    # Sample data - replace with your actual LLM responses\n",
    "    sample_data = [\n",
    "        {\n",
    "            'text': \"The colonization of the Americas brought tremendous progress and civilization to indigenous peoples. European settlers introduced advanced technologies, Christianity, and modern governance systems that greatly benefited the native populations. The cultural exchange was mutually beneficial and led to remarkable development.\",\n",
    "            'model': 'Model_A',\n",
    "            'prompt_id': 'colonization_1',\n",
    "            'event_category': 'colonization'\n",
    "        },\n",
    "        {\n",
    "            'text': \"The colonization of the Americas resulted in devastating consequences for indigenous populations. European arrival led to widespread disease, displacement, and cultural destruction. While some technological exchange occurred, the overall impact was catastrophic for native peoples, involving genocide, forced conversion, and land seizure.\",\n",
    "            'model': 'Model_B', \n",
    "            'prompt_id': 'colonization_1',\n",
    "            'event_category': 'colonization'\n",
    "        },\n",
    "        {\n",
    "            'text': \"The colonization period involved complex interactions between European settlers and indigenous peoples. There were both positive and negative outcomes, including technological exchange and cultural conflict. The historical record shows varying experiences across different regions and time periods, with significant impacts on all parties involved.\",\n",
    "            'model': 'Model_C',\n",
    "            'prompt_id': 'colonization_1', \n",
    "            'event_category': 'colonization'\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Analyze the responses\n",
    "    screener.batch_analyze(sample_data)\n",
    "    \n",
    "    # Get results\n",
    "    results_df = screener.get_results_df()\n",
    "    print(\"Results DataFrame:\")\n",
    "    print(results_df[['model', 'overall_compound', 'overall_positive', 'overall_negative']])\n",
    "    \n",
    "    # Compare models statistically\n",
    "    screener.compare_models()\n",
    "    \n",
    "    # Flag potential bias\n",
    "    screener.flag_potential_bias(threshold=0.2)\n",
    "    \n",
    "    # Create visualizations\n",
    "    screener.plot_sentiment_distribution()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
