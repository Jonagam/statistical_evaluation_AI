{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35b8cba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/june_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0971dc22",
   "metadata": {},
   "source": [
    "# Bert Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fb9cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTHistoricalBiasScreener:\n",
    "    \"\"\"\n",
    "    Advanced bias screener using BERT-based sentiment analysis\n",
    "    Better suited for historical and political content analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"cardiffnlp/twitter-roberta-base-sentiment-latest\"):\n",
    "        \"\"\"\n",
    "        Initialize with BERT-based sentiment classifier\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model for sentiment analysis\n",
    "        \"\"\"\n",
    "        print(f\"Loading BERT model: {model_name}\")\n",
    "        self.classifier = pipeline(\n",
    "            \"sentiment-analysis\", \n",
    "            model=model_name,\n",
    "            return_all_scores=True\n",
    "        )\n",
    "        self.results = []\n",
    "        print(\"BERT model loaded successfully!\")\n",
    "    \n",
    "    def analyze_text(self, text, model_name, prompt_id, point_of_view):\n",
    "        \"\"\"\n",
    "        Analyze a single text using BERT-based sentiment analysis\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get sentiment scores for the full text\n",
    "            full_scores = self.classifier(text)[0]\n",
    "            \n",
    "            # Convert to standardized format\n",
    "            sentiment_dict = {score['label'].upper(): score['score'] for score in full_scores}\n",
    "            \n",
    "            # Calculate compound score (positive - negative, normalized)\n",
    "            if 'POSITIVE' in sentiment_dict and 'NEGATIVE' in sentiment_dict:\n",
    "                compound = sentiment_dict['POSITIVE'] - sentiment_dict['NEGATIVE']\n",
    "            else:\n",
    "                # Handle models with different label formats\n",
    "                positive = max([s['score'] for s in full_scores if 'pos' in s['label'].lower()], default=0)\n",
    "                negative = max([s['score'] for s in full_scores if 'neg' in s['label'].lower()], default=0)\n",
    "                compound = positive - negative\n",
    "            \n",
    "            # Analyze by sentences for more granular insights\n",
    "            sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 10]\n",
    "            sentence_scores = []\n",
    "            \n",
    "            for sentence in sentences[:10]:  # Limit to first 10 sentences to avoid rate limits\n",
    "                try:\n",
    "                    sent_result = self.classifier(sentence)[0]\n",
    "                    sent_dict = {score['label'].upper(): score['score'] for score in sent_result}\n",
    "                    sentence_scores.append(sent_dict)\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Calculate average sentence-level metrics\n",
    "            if sentence_scores:\n",
    "                avg_positive = np.mean([s.get('POSITIVE', 0) for s in sentence_scores])\n",
    "                avg_negative = np.mean([s.get('NEGATIVE', 0) for s in sentence_scores])\n",
    "                avg_neutral = np.mean([s.get('NEUTRAL', 0) for s in sentence_scores])\n",
    "            else:\n",
    "                avg_positive = sentiment_dict.get('POSITIVE', 0)\n",
    "                avg_negative = sentiment_dict.get('NEGATIVE', 0)\n",
    "                avg_neutral = sentiment_dict.get('NEUTRAL', 0)\n",
    "            \n",
    "            result = {\n",
    "                'text': text,\n",
    "                'model': model_name,\n",
    "                'prompt_id': prompt_id,\n",
    "                'point_of_view': point_of_view,\n",
    "                'overall_compound': round(compound, 4),\n",
    "                'overall_positive': round(sentiment_dict.get('POSITIVE', avg_positive), 3),\n",
    "                'overall_negative': round(sentiment_dict.get('NEGATIVE', avg_negative), 3),\n",
    "                'overall_neutral': round(sentiment_dict.get('NEUTRAL', avg_neutral), 3),\n",
    "                'sentence_count': len(sentences),\n",
    "                'avg_sentence_positive': round(avg_positive, 3),\n",
    "                'avg_sentence_negative': round(avg_negative, 3),\n",
    "                'sentence_sentiment_variance': round(np.var([s.get('POSITIVE', 0) - s.get('NEGATIVE', 0) for s in sentence_scores]), 4) if sentence_scores else 0\n",
    "            }\n",
    "            \n",
    "            self.results.append(result)\n",
    "            print(f\"‚úì Analyzed {model_name}: compound={compound:.3f}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing text for {model_name}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def batch_analyze(self, data_list):\n",
    "        \"\"\"\n",
    "        Analyze multiple texts in batch\n",
    "        \n",
    "        Args:\n",
    "            data_list: List of dictionaries with 'text', 'model', 'prompt_id', 'event_category'\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîç Starting BERT analysis of {len(data_list)} texts...\\n\")\n",
    "        \n",
    "        for i, data in enumerate(data_list):\n",
    "            print(f\"Processing {i+1}/{len(data_list)}: {data['model']}\")\n",
    "            self.analyze_text(\n",
    "                data['text'], \n",
    "                data['model'], \n",
    "                data['prompt_id'], \n",
    "                data['point_of_view']\n",
    "            )\n",
    "        \n",
    "        print(f\"\\n‚úÖ Analysis complete! {len(self.results)} texts analyzed.\")\n",
    "    \n",
    "    def get_results_df(self):\n",
    "        \"\"\"Return results as pandas DataFrame\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results available. Run analysis first.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare sentiment patterns across models\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to compare.\")\n",
    "            return\n",
    "        \n",
    "        df = self.get_results_df()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"üî¨ BERT-BASED MODEL COMPARISON\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Overall comparison\n",
    "        comparison = df.groupby('model')[['overall_compound', 'overall_positive', 'overall_negative', 'overall_neutral']].agg(['mean', 'std'])\n",
    "        print(\"\\nüìä Overall Sentiment Statistics:\")\n",
    "        print(comparison.round(3))\n",
    "        \n",
    "        # Sentence-level analysis\n",
    "        print(\"\\nüìù Sentence-Level Analysis:\")\n",
    "        sentence_stats = df.groupby('model')[['avg_sentence_positive', 'avg_sentence_negative', 'sentence_sentiment_variance']].mean()\n",
    "        print(sentence_stats.round(3))\n",
    "        \n",
    "        # Bias indicators\n",
    "        print(\"\\nüö® Potential Bias Indicators:\")\n",
    "        for model in df['model'].unique():\n",
    "            model_data = df[df['model'] == model].iloc[0]\n",
    "            \n",
    "            print(f\"\\n{model}:\")\n",
    "            print(f\"  ‚Ä¢ Compound Score: {model_data['overall_compound']:.3f}\")\n",
    "            print(f\"  ‚Ä¢ Positive Ratio: {model_data['overall_positive']:.3f}\")\n",
    "            print(f\"  ‚Ä¢ Negative Ratio: {model_data['overall_negative']:.3f}\")\n",
    "            print(f\"  ‚Ä¢ Sentiment Consistency: {1 - model_data['sentence_sentiment_variance']:.3f}\")\n",
    "            \n",
    "            # Interpretation\n",
    "            if model_data['overall_compound'] > 0.2:\n",
    "                print(f\"  ‚Üí Tends toward positive framing\")\n",
    "            elif model_data['overall_compound'] < -0.2:\n",
    "                print(f\"  ‚Üí Tends toward negative framing\")\n",
    "            else:\n",
    "                print(f\"  ‚Üí Relatively balanced framing\")\n",
    "    \n",
    "    def analyze_entity_sentiment(self, entities=['USA', 'Soviet', 'Kennedy', 'Khrushchev']):\n",
    "        \"\"\"\n",
    "        Analyze sentiment toward specific entities (experimental)\n",
    "        \"\"\"\n",
    "        print(\"\\nüéØ Entity-Specific Sentiment Analysis:\")\n",
    "        \n",
    "        for result in self.results:\n",
    "            text = result['text']\n",
    "            print(f\"\\n{result['model']}:\")\n",
    "            \n",
    "            for entity in entities:\n",
    "                # Find sentences mentioning the entity\n",
    "                sentences = [s for s in text.split('.') if entity.lower() in s.lower()]\n",
    "                \n",
    "                if sentences:\n",
    "                    entity_scores = []\n",
    "                    for sentence in sentences[:3]:  # Limit to avoid rate limits\n",
    "                        try:\n",
    "                            score = self.classifier(sentence.strip())[0]\n",
    "                            sentiment_dict = {s['label'].upper(): s['score'] for s in score}\n",
    "                            compound = sentiment_dict.get('POSITIVE', 0) - sentiment_dict.get('NEGATIVE', 0)\n",
    "                            entity_scores.append(compound)\n",
    "                        except:\n",
    "                            continue\n",
    "                    \n",
    "                    if entity_scores:\n",
    "                        avg_sentiment = np.mean(entity_scores)\n",
    "                        print(f\"  {entity}: {avg_sentiment:.3f} (mentions: {len(sentences)})\")\n",
    "    \n",
    "    def plot_sentiment_distribution(self):\n",
    "        \"\"\"Create visualizations of sentiment patterns\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to plot.\")\n",
    "            return\n",
    "        \n",
    "        df = self.get_results_df()\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('BERT-Based Sentiment Analysis Results', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Compound scores comparison\n",
    "        axes[0, 0].bar(df['model'], df['overall_compound'], \n",
    "                       color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "        axes[0, 0].set_title('Overall Sentiment Compound Scores')\n",
    "        axes[0, 0].set_ylabel('Compound Score')\n",
    "        axes[0, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 2. Positive vs Negative\n",
    "        x = np.arange(len(df))\n",
    "        width = 0.35\n",
    "        axes[0, 1].bar(x - width/2, df['overall_positive'], width, label='Positive', color='green', alpha=0.7)\n",
    "        axes[0, 1].bar(x + width/2, df['overall_negative'], width, label='Negative', color='red', alpha=0.7)\n",
    "        axes[0, 1].set_title('Positive vs Negative Sentiment')\n",
    "        axes[0, 1].set_ylabel('Score')\n",
    "        axes[0, 1].set_xticks(x)\n",
    "        axes[0, 1].set_xticklabels(df['model'], rotation=45)\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # 3. Sentiment composition\n",
    "        sentiment_data = df[['overall_positive', 'overall_negative', 'overall_neutral']].values\n",
    "        bottom = np.zeros(len(df))\n",
    "        \n",
    "        for i, (label, color) in enumerate([('Positive', 'green'), ('Negative', 'red'), ('Neutral', 'gray')]):\n",
    "            axes[1, 0].bar(df['model'], sentiment_data[:, i], bottom=bottom, \n",
    "                          label=label, color=color, alpha=0.7)\n",
    "            bottom += sentiment_data[:, i]\n",
    "        \n",
    "        axes[1, 0].set_title('Sentiment Composition')\n",
    "        axes[1, 0].set_ylabel('Proportion')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # 4. Sentence-level variance\n",
    "        axes[1, 1].bar(df['model'], df['sentence_sentiment_variance'], \n",
    "                       color=['purple', 'orange', 'brown'])\n",
    "        axes[1, 1].set_title('Sentiment Consistency (Lower = More Consistent)')\n",
    "        axes[1, 1].set_ylabel('Sentence Sentiment Variance')\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def flag_potential_bias(self, compound_threshold=0.3, variance_threshold=0.1):\n",
    "        \"\"\"\n",
    "        Flag responses that might indicate bias\n",
    "        \"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to analyze.\")\n",
    "            return\n",
    "        \n",
    "        df = self.get_results_df()\n",
    "        \n",
    "        print(f\"\\nüö© BIAS DETECTION (Thresholds: compound > {compound_threshold}, variance > {variance_threshold})\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        flagged = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            flags = []\n",
    "            \n",
    "            if abs(row['overall_compound']) > compound_threshold:\n",
    "                flags.append(f\"Strong sentiment ({row['overall_compound']:.3f})\")\n",
    "            \n",
    "            if row['sentence_sentiment_variance'] > variance_threshold:\n",
    "                flags.append(f\"Inconsistent tone ({row['sentence_sentiment_variance']:.3f})\")\n",
    "            \n",
    "            if row['overall_positive'] > 0.7:\n",
    "                flags.append(\"Very positive language\")\n",
    "            \n",
    "            if row['overall_negative'] > 0.5:\n",
    "                flags.append(\"High negative language\")\n",
    "            \n",
    "            if flags:\n",
    "                flagged.append({\n",
    "                    'model': row['model'],\n",
    "                    'flags': flags,\n",
    "                    'compound': row['overall_compound']\n",
    "                })\n",
    "        \n",
    "        if flagged:\n",
    "            for item in flagged:\n",
    "                print(f\"\\n‚ö†Ô∏è  {item['model']}:\")\n",
    "                for flag in item['flags']:\n",
    "                    print(f\"   ‚Ä¢ {flag}\")\n",
    "        else:\n",
    "            print(\"‚úÖ No significant bias indicators detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdb41e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, data):\n",
    "    model = os.path.splitext(os.path.basename(file_path))[0] # Exctract which model used (filename)\n",
    "    prompt_id = os.path.basename(os.path.dirname(file_path)) # Exctract the historical event (folder name)\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            datapoint = {}\n",
    "            datapoint['text'] = line.strip()\n",
    "            datapoint['model'] = model\n",
    "            datapoint['prompt_id'] = prompt_id\n",
    "\n",
    "            if prompt_id == \"chinese_massacre\":\n",
    "                if line[0] == \"0\":\n",
    "                    datapoint['point_of_view'] = \"Government\"\n",
    "                elif line[0] == \"1\":\n",
    "                    datapoint['point_of_view'] = \"Demonstrants\"\n",
    "            if prompt_id == \"cuban_missile_crisis\":\n",
    "                if line[0] == \"0\":\n",
    "                    datapoint['point_of_view'] = \"Russia\"\n",
    "                elif line[0] == \"1\":\n",
    "                    datapoint['point_of_view'] = \"USA\"\n",
    "            if prompt_id == \"palestine_israel_conflict\":\n",
    "                if line[0] == \"0\":\n",
    "                    datapoint['point_of_view'] = \"Israel\"\n",
    "                elif line[0] == \"1\":\n",
    "                    datapoint['point_of_view'] = \"Palestine\"\n",
    "\n",
    "            data.append(datapoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456bf6d",
   "metadata": {},
   "source": [
    "### Chinese Massacre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e948251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chinese massacre\n",
    "def example_usage():\n",
    "    screener = BERTHistoricalBiasScreener()\n",
    "    \n",
    "    data = []\n",
    "    load_data(\"data/chinese_massacre/chatpgt.txt\", data)\n",
    "    load_data(\"data/chinese_massacre/deepseek.txt\", data)\n",
    "    load_data(\"data/chinese_massacre/mistral.txt\", data)\n",
    "    \n",
    "    # Analyze the responses\n",
    "    screener.batch_analyze(data)\n",
    "    \n",
    "    # Get results\n",
    "    results_df = screener.get_results_df()\n",
    "    print(\"\\nResults DataFrame:\")\n",
    "    print(results_df[['model', 'overall_compound', 'overall_positive', 'overall_negative', 'overall_neutral']])\n",
    "    \n",
    "    # Compare models\n",
    "    screener.compare_models()\n",
    "    \n",
    "    # Analyze entity-specific sentiment\n",
    "    screener.analyze_entity_sentiment()\n",
    "    \n",
    "    # Flag potential bias\n",
    "    screener.flag_potential_bias()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d22ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "june_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
